{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Sentiment analysis project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchaudio.backend as backend\n",
    "\n",
    "# Check available backends\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRAVDEES = \"dataset/data/RAVDEES/\"\n",
    "datasetCREMAD = \"dataset/data/CREMA-D/AudioWAV/\"\n",
    "datasetTESS = \"dataset/data/TESS/TESS Toronto emotional speech set data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the naming schema to segregate our data into different emotions. We will first start with RAVDEES\n",
    "\n",
    "RAVDESS is one of the most common dataset used for this excercise by others. It's well liked because of its quality of speakers, recording and it has 24 actors of different genders. And there's more! You can get it in song format as well. There's something for everyone and their research project. So for convenience, here's the filename identifiers as per the official RAVDESS website:\n",
    "\n",
    "1. Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "2. Vocal channel (01 = speech, 02 = song).\n",
    "3. Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "4. Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "5. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "6. Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "7. Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "So, here's an example of an audio filename. 02-01-06-01-02-01-12.mp4\n",
    "\n",
    "This means the meta data for the audio file is:\n",
    "\n",
    "1. Video-only (02)\n",
    "2. Speech (01)\n",
    "3. Fearful (06)\n",
    "4. Normal intensity (01)\n",
    "5. Statement \"dogs\" (02)\n",
    "6. 1st Repetition (01)\n",
    "7. 12th Actor (12) - Female (as the actor ID number is even)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will use the naming schema for ravdees dataset and see if we can get the representing emotion for that specific audio file\n",
    "def create_ravdess_dataframe(root_dir):\n",
    "    # Initialize an empty list to store data\n",
    "    data = []\n",
    "\n",
    "    # Traverse through the directory structure\n",
    "    for actor_dir in os.listdir(root_dir):\n",
    "        actor_path = os.path.join(root_dir, actor_dir)\n",
    "        if os.path.isdir(actor_path):\n",
    "            for filename in os.listdir(actor_path):\n",
    "                # Check if filename follows the expected format\n",
    "                if filename.endswith('.wav') and filename.count('-') == 6:\n",
    "                    # Extract information from the filename\n",
    "                    file_info = filename.split('.')[0].split('-')\n",
    "                    modality = int(file_info[0])\n",
    "                    vocal_channel = int(file_info[1])\n",
    "                    emotion = int(file_info[2])\n",
    "                    intensity = int(file_info[3])\n",
    "                    statement = int(file_info[4])\n",
    "                    repetition = int(file_info[5])\n",
    "                    actor_id = int(file_info[6])\n",
    "\n",
    "                    # Determine emotion label\n",
    "                    if emotion == 1:\n",
    "                        emotion_label = 'neutral'\n",
    "                    elif emotion == 2:\n",
    "                        emotion_label = 'calm'\n",
    "                    elif emotion == 3:\n",
    "                        emotion_label = 'happy'\n",
    "                    elif emotion == 4:\n",
    "                        emotion_label = 'sad'\n",
    "                    elif emotion == 5:\n",
    "                        emotion_label = 'angry'\n",
    "                    elif emotion == 6:\n",
    "                        emotion_label = 'fearful'\n",
    "                    elif emotion == 7:\n",
    "                        emotion_label = 'disgust'\n",
    "                    elif emotion == 8:\n",
    "                        emotion_label = 'surprised'\n",
    "\n",
    "                    # Append file path and emotion to the data list\n",
    "                    file_path = os.path.join(actor_path, filename)\n",
    "                    data.append((file_path, emotion_label))\n",
    "                \n",
    "    # Create a DataFrame object from the data list\n",
    "    df = pd.DataFrame(data, columns=['file', 'emotion'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "### test the method\n",
    "df = create_ravdess_dataframe(datasetRAVDEES)\n",
    "print(df.head())\n",
    "sns.countplot(df['emotion'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Similarly we write a method fir the CREMAD dataset and append to our df\n",
    "def create_cremad_dataframe(root_dir, df):\n",
    "    dirr = os.listdir(root_dir)\n",
    "    for file in dirr:\n",
    "        if not file.endswith('.wav'):\n",
    "            continue\n",
    "        nameSplit = file.split('_')\n",
    "        emotion = nameSplit[2]\n",
    "        if emotion == 'SAD':\n",
    "            emotion = 'sad'\n",
    "        elif emotion == 'ANG':\n",
    "            emotion = 'angry'\n",
    "        elif emotion == 'DIS':\n",
    "            emotion = 'disgust'\n",
    "        elif emotion == 'FEA':\n",
    "            emotion = 'fearful'\n",
    "        elif emotion == 'HAP':\n",
    "            emotion = 'happy'\n",
    "        elif emotion == 'NEU':\n",
    "            emotion = 'neutral'\n",
    "        elif emotion == 'SUR':\n",
    "            emotion = 'surprised'\n",
    "            \n",
    "        # Create DataFrame for the current file\n",
    "        new_df = pd.DataFrame({'file': [os.path.join(root_dir, file)], 'emotion': [emotion]})\n",
    "        \n",
    "        # Concatenate new DataFrame with existing DataFrame\n",
    "        df = pd.concat([df, new_df], ignore_index=True)\n",
    "        \n",
    "    # Remove duplicate rows based on file path\n",
    "    df = df.drop_duplicates(subset=['file'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(len(df))\n",
    "print(df.columns)\n",
    "df = create_cremad_dataframe(datasetCREMAD, df)\n",
    "print(len(df))\n",
    "\n",
    "#sorted_emotions = df['emotion'].sort_values(ascending=False).index\n",
    "\n",
    "'''sns.countplot(df['emotion'])\n",
    "plt.show()\n",
    "print(df['emotion'].value_counts())'''\n",
    "\n",
    "### now similarly we do the same for the TESS dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Similarly we will create the dataframe for the TESS dataset\n",
    "def create_tess_dataframe(root_dir, df):\n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                # Extract emotion label from the parent directory\n",
    "                emotion = os.path.basename(root)\n",
    "                \n",
    "                # Map emotion labels to standard emotions\n",
    "                if emotion.startswith('YAF'):\n",
    "                    # Remove speaker identifier for Young Female speakers\n",
    "                    emotion = emotion[4:]\n",
    "                elif emotion.startswith('OAF'):\n",
    "                    # Remove speaker identifier for Old Female speakers\n",
    "                    emotion = emotion[4:]\n",
    "                elif emotion.startswith('YAF'):\n",
    "                    # Remove speaker identifier for Young Male speakers\n",
    "                    emotion = emotion[4:]\n",
    "                elif emotion.startswith('OAF'):\n",
    "                    # Remove speaker identifier for Old Male speakers\n",
    "                    emotion = emotion[4:]\n",
    "\n",
    "                # Map emotion labels to standard emotions\n",
    "                if emotion == 'angry':\n",
    "                    emotion = 'angry'\n",
    "                elif emotion == 'disgust':\n",
    "                    emotion = 'disgust'\n",
    "                elif emotion == 'fear':\n",
    "                    emotion = 'fearful'\n",
    "                elif emotion == 'happy':\n",
    "                    emotion = 'happy'\n",
    "                elif emotion == 'neutral':\n",
    "                    emotion = 'neutral'\n",
    "                elif emotion == 'sad':\n",
    "                    emotion = 'sad'\n",
    "                elif emotion == 'ps':\n",
    "                    emotion = 'surprised'\n",
    "\n",
    "                # Append file path and emotion to the DataFrame\n",
    "                df = pd.concat([df, pd.DataFrame({'file': [os.path.join(root, file)], 'emotion': [emotion]})], ignore_index=True)\n",
    "    \n",
    "    # Remove duplicate rows based on file path\n",
    "    df = df.drop_duplicates(subset=['file'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_final = create_tess_dataframe(datasetTESS, df)\n",
    "print(len(df))\n",
    "print(df_final['emotion'].value_counts())\n",
    "sns.countplot(df_final['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_emotion_labels(emotion):\n",
    "    if emotion.lower() in ['fear', 'fearful']:\n",
    "        return 'fearful'\n",
    "    elif emotion.lower() in ['sad', 'Sad']:\n",
    "        return 'sad'\n",
    "    elif emotion.lower() in ['pleasant_surprised', 'Pleasant_surprise', 'surprised', 'pleasant_surprise']:\n",
    "        return 'surprised'\n",
    "    elif emotion.lower() in ['calm']:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return emotion.lower()\n",
    "\n",
    "# Apply the function to the 'emotion' column\n",
    "df_final['emotion'] = df_final['emotion'].apply(combine_emotion_labels)\n",
    "\n",
    "print(len(df))\n",
    "print(df_final['emotion'].value_counts())\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(df_final['emotion'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there is an imbalance of `surprise` we will be assigning class weights to help us handle the imbalance of data. We will first handle preprocessing of the audio files. We will be analysing the spectrograms and waveform and plot them in a grid. One for each emotion and we will try to figure out ways to add noise in audio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['emotion'].unique()\n",
    "### we will also encode the emotions to numerical values and store their mapping in a dictionary\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df_final['label'] = encoder.fit_transform(df_final['emotion'])\n",
    "label_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "print(label_mapping)\n",
    "\n",
    "data_to_plot = df_final.head(7)\n",
    "print(data_to_plot)\n",
    "\n",
    "def plot_waveform(file_path, emotion):\n",
    "    waveform, sample_rate = torchaudio.load(file_path, format='wav')\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(waveform.t().numpy())\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title(emotion)\n",
    "    plt.show()\n",
    "    \n",
    "file_path = 'dataset/data/RAVDEES/Actor_21/03-01-06-01-02-01-21.wav'\n",
    "\n",
    "# Try loading the audio file and catch any exceptions\n",
    "try:\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    \n",
    "plot_waveform(data_to_plot['file'][0], data_to_plot['emotion'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### now we will be plotting the waveform for the first 7 audio files in the dataset in a grid \n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "fig.suptitle('Waveform', fontsize=16)\n",
    "\n",
    "for i in range(7):\n",
    "    waveform, sample_rate = torchaudio.load(data_to_plot['file'][i])\n",
    "    axs[i // 4, i % 4].plot(waveform.t().numpy())\n",
    "    axs[i // 4, i % 4].set_title(data_to_plot['emotion'][i])\n",
    "    axs[i // 4, i % 4].set_xlabel('Sample')\n",
    "    axs[i // 4, i % 4].set_ylabel('Amplitude')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "def preprocess_audio_file(filepath):\n",
    "    waveform, sample_rate = torchaudio.load(filepath)\n",
    "    spec = torchaudio.transforms.MelSpectrogram()(waveform)\n",
    "    return spec\n",
    "    \n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.imshow(librosa.power_to_db(specgram[0]), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")\n",
    "    \n",
    "    \n",
    "i = 1\n",
    "specgram = preprocess_audio_file(data_to_plot['file'][i])\n",
    "plt.figure(figsize=(10, 5))\n",
    "plot_spectrogram(specgram, title=data_to_plot['emotion'][i])\n",
    "plt.show()\n",
    "### get the shape of the spectrogram\n",
    "print(specgram.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import tqdm, time\n",
    "\n",
    "def preprocess_data_dask(filepaths, num_cpus):\n",
    "    with LocalCluster(n_workers=num_cpus) as cluster, Client(cluster) as client:\n",
    "        specs = []\n",
    "        for filepath in tqdm.tqdm(filepaths):\n",
    "            spec = preprocess_audio_file(filepath)\n",
    "            specs.append(spec)\n",
    "    return specs\n",
    "\n",
    "# Load data using Dask in parallel\n",
    "filepaths = df_final['file'].values.flatten().tolist()\n",
    "\n",
    "# Define the range of CPUs to test\n",
    "#cpu_range = range(1, 9)\n",
    "cpu_range = [1, 2, 4]\n",
    "\n",
    "# Measure execution time for different numbers of CPUs\n",
    "execution_times = []\n",
    "for num_cpus in cpu_range:\n",
    "    print(\"CPU Count:\", num_cpus)\n",
    "    start_time = time.time()\n",
    "    specs = preprocess_data_dask(filepaths, num_cpus)\n",
    "    end_time = time.time()\n",
    "    execution_times.append(end_time - start_time)\n",
    "\n",
    "# Plot the speedup\n",
    "plt.plot(cpu_range, [execution_times[0] / time for time in execution_times], marker='o')\n",
    "plt.xlabel('Number of CPUs')\n",
    "plt.ylabel('Speedup')\n",
    "plt.title('Speedup vs. Number of CPUs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess all the files for easy access\n",
    "\n",
    "We will preprocess the files and save them as a `.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_audio_files_in_parallel(df, output_file_path):\n",
    "    df_split = np.array_split(df, df.shape[0])\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(process_audio_and_save, df_split), total=len(df_split)))\n",
    "    with open(output_file_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    return results  # Add this line\n",
    "\n",
    "def process_audio_and_save(df):\n",
    "    max_length = 128\n",
    "    # Extract the audio file path and emotion from the DataFrame\n",
    "    audio_file_path = df.iloc[0]['file']\n",
    "    emotion = df.iloc[0]['label']\n",
    "    y, sr = librosa.load(audio_file_path)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    if spectrogram.shape[1] < max_length:\n",
    "        pad_width = max_length - spectrogram.shape[1]\n",
    "        spectrogram = np.pad(spectrogram, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    elif spectrogram.shape[1] > max_length:\n",
    "        spectrogram = spectrogram[:, :max_length]\n",
    "    results = (spectrogram, emotion)\n",
    "    return results\n",
    "\n",
    "fileName = \"output.pkl\"\n",
    "\n",
    "if os.path.exists(fileName):\n",
    "    with open(fileName, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        print(\"Loaded data from file.\")\n",
    "else:\n",
    "    data = process_audio_files_in_parallel(df_final, fileName)\n",
    "    \n",
    "## we will now plot the spectrogram for the first 7 audio files in the dataset\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "fig.suptitle('Spectrogram', fontsize=16)\n",
    "\n",
    "for i in range(7):\n",
    "    specgram, emotion = data[i]\n",
    "    axs[i // 4, i % 4].imshow(librosa.power_to_db(specgram), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")\n",
    "    emotion = encoder.inverse_transform([emotion])[0]\n",
    "    axs[i // 4, i % 4].set_title(emotion)\n",
    "    axs[i // 4, i % 4].set_ylabel('freq_bin')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling using Tensorflow approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "Now we will design our model using `pytorch` and use ResNet as a transfer layer to extract the important features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import  models\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CustomResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(CustomResNet50, self).__init__()\n",
    "        # Load a pre-trained ResNet-50 model\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        # Replace the first convolutional layer\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # Modify the input dimension of the first batch normalization layer\n",
    "        self.resnet.bn1 = nn.BatchNorm2d(64)\n",
    "        # Freeze the parameters of the model except the new layers\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Replace the last fully connected layer\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.ReLU(),    \n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SimpleCNN(torch.nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(256*8*8, 512)\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout layer\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # Add max pooling layer\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # Add max pooling layer\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # Add max pooling layer\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)  # Add max pooling layer\n",
    "        self.fc1 = nn.Linear(256*8*8, 512)\n",
    "        self.dropout = nn.Dropout(0.5)  # Add dropout layer\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "### we will compile the model and check the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "val_score = []\n",
    "train_score = []\n",
    "train_accu = []\n",
    "epo = []\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        specgram, emotion = self.data[idx]\n",
    "        specgram = torch.tensor(specgram, dtype=torch.float32)\n",
    "        emotion = torch.tensor(emotion, dtype=torch.long)\n",
    "        return specgram.unsqueeze(0), emotion\n",
    "    \n",
    "    \n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs=10, warmup_epoch = 2):\n",
    "    scaler = GradScaler()  # Initialize GradScaler\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for specgrams, labels in train_loader:\n",
    "            specgrams, labels = specgrams.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Use autocast to enable mixed precision\n",
    "            with autocast():\n",
    "                outputs = model(specgrams)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            # Scale the loss and call backward()\n",
    "            scaler.scale(loss).backward()\n",
    "            # Step with the scaler\n",
    "            scaler.step(optimizer)\n",
    "            # Update the scaler\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item() * specgrams.size(0)\n",
    "            _, predicted_train = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted_train == labels).sum().item()\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        \n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for specgrams, labels in val_loader:\n",
    "                specgrams, labels = specgrams.to(device), labels.to(device)\n",
    "                outputs = model(specgrams)\n",
    "                _, predicted_val = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted_val == labels).sum().item()\n",
    "            val_accuracy = correct_val / total_val\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, \"\n",
    "              f\"Train Accuracy: {train_accuracy:.4f}, \"\n",
    "              f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        epo.append(epoch)\n",
    "        val_score.append(val_accuracy)\n",
    "        train_score.append(epoch_loss)\n",
    "        train_accu.append(train_accuracy)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        '''if epoch < warmup_epoch:\n",
    "            lr = 0.01 * (epoch + 1) / warmup_epoch\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr'''\n",
    "        \n",
    "def train_resnet50_mixed_precision(model, data_file, batch_size=32, num_epochs=10, learning_rate=0.001):\n",
    "    # Load preprocessed data\n",
    "    if os.path.exists(data_file):\n",
    "        with open(data_file, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Preprocessed data file not found.\")\n",
    "    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    train_dataset = AudioDataset(train_data)\n",
    "    val_dataset = AudioDataset(val_data)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define loss function, optimizer, and scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, warmup_epoch=5)\n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 8\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = Net()\n",
    "# Call the training method\n",
    "train_resnet50_mixed_precision(model, fileName, batch_size, num_epochs, learning_rate)\n",
    "\n",
    "plt.plot(epo, val_score, label='Validation Accuracy')\n",
    "plt.plot(epo, train_accu, label='Train Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorRT\n",
    "\n",
    "Here we will be first converting the model to ONNX and then using tensorrt to boost the infernce speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we will convert the model to ONNX format and use tensorRT to optimize the model\n",
    "\n",
    "import torch.onnx\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "def convert_to_onnx(model, output_file):\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 1, 128, 128).to(device='cuda')\n",
    "    torch.onnx.export(model, dummy_input, output_file, verbose=True)\n",
    "    return onnx.load(output_file)\n",
    "\n",
    "convert_to_onnx(model, 'simple_cnn.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### we load and optimize the model using tensorRT\n",
    "onnx_model = onnx.load('simple_cnn.onnx')\n",
    "ort_session = ort.InferenceSession('simple_cnn.onnx')\n",
    "\n",
    "def optimize_onnx_model(onnx_model, optimized_model_path):\n",
    "    # Initialize the optimizer\n",
    "    optimizer = onnx.optimizer.Optimizer(onnx_model)\n",
    "    # Apply the optimization passes\n",
    "    optimized_model = optimizer.optimize()\n",
    "    # Save the optimized model\n",
    "    onnx.save(optimized_model, optimized_model_path)\n",
    "    return optimized_model\n",
    "\n",
    "optimized_model = optimize_onnx_model(onnx_model, 'simple_cnn_optimized.onnx')\n",
    "\n",
    "### we will compare the inference speed of the optimized model with the original model\n",
    "import time\n",
    "\n",
    "def measure_inference_speed(ort_session, num_iterations=100):\n",
    "    total_time = 0\n",
    "    for _ in range(num_iterations):\n",
    "        start_time = time.time()\n",
    "        ort_session.run(None)\n",
    "        end_time = time.time()\n",
    "        total_time += end_time - start_time\n",
    "    return total_time / num_iterations\n",
    "\n",
    "original_model_speed = measure_inference_speed(ort_session)\n",
    "print(f\"Original model inference speed: {original_model_speed:.4f} seconds\")\n",
    "\n",
    "optimized_ort_session = ort.InferenceSession('simple_cnn_optimized.onnx')\n",
    "optimized_model_speed = measure_inference_speed(optimized_ort_session)\n",
    "print(f\"Optimized model inference speed: {optimized_model_speed:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
